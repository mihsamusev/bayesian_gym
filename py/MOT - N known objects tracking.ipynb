{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracking n objects\n",
    "The SOT model developed previously was only capable of tracking a single object and estimate its state $x_k$ in clutter and missed detections. Generalization of SOT is the ability to track $n$ objects and estimate the state matrix $X_k = [x_k^1, x_k^2,...,x_k^n]$ where $n$ is assumed to be both known and constant. The clutter and the missed detections will be included as well. The crux of $n$ object tracking is handling many data association problem. \n",
    "\n",
    "New models required for the problem are:\n",
    "\n",
    "- model for the measurements from all $n$ objects and the clutter\n",
    "- model the motion of all $n$ objects\n",
    "- prior for the states of the $n$ objects\n",
    "- methods for handling the data association\n",
    "\n",
    "As well as new algorithms:\n",
    "\n",
    "- Global Nearest Neigbour (GNN) filter\n",
    "- Joint Probablistic Data Association (JPDA) filter\n",
    "- Multi Hypothesis Tracker (MHT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measurement modelling\n",
    "Similarly to the SOT the measurement matrix  $Z_k = \\Pi(O_k, C_k)$ is a random permutation between the clutter measurements $C_k$ and object detections $O_k$. Similarly to SOT the clutter is Poisson point process with intensity $\\lambda_c(c) = \\bar{\\lambda}_cf_c(c)$ depending on the clutter rate and the spatial PFD. The object detections, however, are now $O_k = [O_k^1,...O_k^i,...,O_k^n]$. Nevertheless, for $O_k^i$ same measurement model as in SOT applies, namely:\n",
    "$$\n",
    "\\begin{cases}\n",
    "O_k^i = [] & \\text{ with probability } 1 - P^D(x_k^i) \\\\\n",
    "O_k^i = o_k^i & \\text{ with probability } P^D(x_k^i) \\text{ and likelihood } g_k(o_k^i|x_k^i)\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "How does the n - object measurement likelihood $p(Z_k|X_k)$ looks in this case? Similarly to the SOT, the measurement likelihood can be build introducing the hypotheses variable and applying the law of total probability.\n",
    "\n",
    "$$\n",
    "p(Z|x) = p(Z,m|x) = \\sum_{\\theta=0}^{m}p(Z,m,\\theta|x) = \\sum_{\\theta=0}^{m}p(Z|m,\\theta,x)p(\\theta,m|x)\n",
    "$$\n",
    "\n",
    "Lets take it part by part again and derive the components\n",
    "\n",
    "- Association conditioned measurement model $p(Z|m,\\theta,x)$\n",
    "- Assication prior $p(\\theta,m|x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data association variable $\\theta$\n",
    "For the measurements  $Z_k = [Z_k^1,...Z_k^i,...,Z_k^n]$ the $\\theta_k^i$ is the association for the object with state $x_k^i$ such that:\n",
    "\n",
    "$$ \\theta_k^i =\n",
    "\\begin{cases}\n",
    "j & \\text{ if object } i \\text{ is associated to measurement }j\\\\\n",
    "0 & \\text{ if object } i \\text{ is undetected}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The associations for all variables become $\\theta_k = [\\theta_k^1,...\\theta_k^i,...,\\theta_k^n]$\n",
    "\n",
    "Example:\n",
    "Two objects $X = [x^1, x^2]$\n",
    "\n",
    "Two measurements $Z = [z^1, z^2]$\n",
    "\n",
    "Example association $\\theta = [1, 0]$ meaning $x^1$ associated to $z^1$ and $x^2$ associated to misdetection.\n",
    "\n",
    "The extension of the set of associations $\\theta_k$ is the set of **valid** associations $\\Theta_k$. $\\theta_k \\in \\Theta_k$ only if two conditions are met:\n",
    "\n",
    "1. Each objcet must be either detected or miss-detected\n",
    "2. Any pair of detected objects cannot be associated to te same measurement (point object assumption)\n",
    "\n",
    "In the following we only consider valid associations, unless otherwise stated. Since we now have more than one object detection, the amount of object detections at time $k$ is denoted $m_k^o$ and the amount of clutter detections is then $m_k^c = m_k - m_k^o$\n",
    "\n",
    "Example:\n",
    "For two measurements $Z = [z^1, z^2]$ and two objects $X = [x^1, x^2]$ there are 7 valid associations $\\theta^i$ in $\\Theta$:\n",
    "\n",
    "| $i$ | $\\theta$ | $O^1$ | $O^2$ | $C$ | $m^o$ | $m^c$ |\n",
    "| --- | --- | --- | --- | --- | --- | --- |\n",
    "| 1 | $[0, 0]$ | $[]$ | $[]$ | $[z^1, z^2]$ | $0$ | $2$ |\n",
    "| 2 | $[1, 0]$ | $[z^1]$ | $[]$ | $[z^2]$ | $1$ | $1$ |\n",
    "| 3 | $[2, 0]$ | $[z^2]$ | $[]$ | $[z^1]$ | $1$ | $1$ |\n",
    "| 4 | $[0, 1]$ | $[]$ | $[z^1]$ | $[z^2]$ | $1$ | $1$ |\n",
    "| 5 | $[0, 2]$ | $[]$ | $[z^2]$ | $[z^1]$ | $1$ | $1$ |\n",
    "| 6 | $[1, 2]$ | $[z^1]$ | $[z^2]$ | $[]$ | $2$ | $1$ |\n",
    "| 7 | $[2, 1]$ | $[z^2]$ | $[z^1]$ | $[]$ | $2$ | $0$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of associations\n",
    "If we have $m$ measurements and $n$ objects then the number of object measurements is $m^o \\in \\{0,1,...,\\min(m,n)\\}$. There is $\\binom{n}{m^o}$ ways to select $m^o$ object measurements from $n$ objects. There is also $\\binom{m}{m^o}$ ways to select $m^o$ object measurements from $m$ measurements. At last there are $m^o!$ ways to associate selected object measurements to selected objects. Total number of data associations is then expressed as:\n",
    "\n",
    "$$\n",
    "N_A(m,n) = \\sum_{m^o=0}^{\\min(m,n)}\\binom{n}{m^o}\\binom{m}{m^o}m^o! = \\sum_{m^o=0}^{\\min(m,n)}\\frac{m!n!}{m^o!(m-m^o)!(n-m^o)!}\n",
    "$$\n",
    "For example for SOT we can calculate the number of associations $N_A(m, 1)$:\n",
    "$$\n",
    "N_A(m,1) = \\binom{1}{0}\\binom{m}{0}0! + \\binom{1}{1}\\binom{m}{1}1! = 1 + m\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_associations(n, m):\n",
    "    '''generates valid association vectors given\n",
    "    n - number of objects\n",
    "    m - nummber of measurements'''\n",
    "\n",
    "def association_data(theta):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Association prior\n",
    "The data association prior $p(\\theta,m|x)$ for SOT and for n object tracking can be expressed as follows. It is helpful to draw the paralels between different terms in both formulas and see what has changed when our assumption about a single object got extended.\n",
    "$$\n",
    "p(\\theta,m|x) = P^D(x)Po(m - 1: \\bar{\\lambda}_c)\\frac{1}{m}\n",
    "\\\\\n",
    "p(\\theta,m|x) = \n",
    "\\underbrace{\\prod_{i:\\theta^i = 0}(1 - P^D(x^i))\\prod_{i:\\theta^i \\neq 0}P^D(x^i)}\n",
    "_{\\text{(1)}}\n",
    "\\underbrace{Po(m^c: \\bar{\\lambda}_c)}\n",
    "_{\\text{(2)}}\n",
    "\\underbrace{\\frac{1}{\\binom{m}{m^o}m^o!}}\n",
    "_{\\text{(3)}}\n",
    "$$\n",
    "\n",
    "where:\n",
    "1. Probability of detectiong a specific set of $m^o$ objects and miss-detecting the rest (Not just detecting $m^o$ out of $m$ objects)\n",
    "2. Probability of $m^c = m - m^o$ clutter detections dictated by the Poisson distribution\n",
    "3. Probability of the specific arangement of data association $\\theta_k$, 1 over number of ways to select $m^o$ detections and associating them to the specific objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Association conditined likelihood\n",
    "The data association conditined likelihood $p(\\theta,m|x)$ is derived using the simplifying assumption that given $\\theta$ and $m$ the measurements are independent. The expression then becomes a product of the clutter likelihoods sampled from clutter pdf and the measurement likelihoods sampled from the corresponding function.\n",
    "\n",
    "$$\n",
    "p(Z|X,\\theta,m) = \\prod_{j:\\nexists \\ \\theta^i = j}f_c(z^j)\\prod_{i:\\theta^i \\neq 0}g(z^{\\theta^i}|x^i)\n",
    "$$\n",
    "\n",
    "where $j:\\nexists \\ \\theta^i = j$ can be read as _indices not contained in the data association variable, or measurements associated to clutter_ and $i:\\theta^i \\neq 0$ as _for all indices of data association variable except the the miss-detection hypothesis, or measurements associated to object detections_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measurement model\n",
    "Combining the two parts derived above we arive at the  $n$ object measurement likelihood.\n",
    "\n",
    "$$\n",
    "p(Z|X) = \\sum_{\\theta \\in \\Theta}\n",
    "\\left[\\prod_{j:\\nexists \\ \\theta^i = j}f_c(z^j)\\prod_{i:\\theta^i \\neq 0}g(z^{\\theta^i}|x^i)\\right]\n",
    "\\left[\\frac{1}{\\binom{m}{m^o}m^o!}Po(m^c|\\bar{\\lambda}_c)\\prod_{i:\\theta^i = 0}(1 - P^D(x^i))\\prod_{i:\\theta^i \\neq 0}P^D(x^i)\\right]\n",
    "$$\n",
    "\n",
    "One can simplify the exression by expand the definition of the Poisson distribution, expand the binomial coeffcient and combine the terms under the products over the same hypotheses to obtain the following:\n",
    "\n",
    "$$\n",
    "p(Z|X) = \\sum_{\\theta \\in \\Theta} \\frac{e^{-\\bar{\\lambda}_c}}{m!}\n",
    "\\prod_{j:\\nexists \\ \\theta^i = j}\\lambda_c(z^j)\n",
    "\\prod_{i:\\theta^i = 0}\\left(1 - P^D(x^i)\\right)\n",
    "\\prod_{i:\\theta^i \\neq 0}P^D(x^i)g(z^{\\theta^i}|x^i)\n",
    "$$\n",
    "\n",
    "Further simplification can be done by smart multiplication by 1 and making the clutter detection part independent of the hypotheses.\n",
    "\n",
    "$$\n",
    "p(Z|X) = \\sum_{\\theta \\in \\Theta} \\frac{e^{-\\bar{\\lambda}_c}}{m!}\n",
    "\\prod_{j=1}^{m}\\lambda_c(z^j)\n",
    "\\prod_{i:\\theta^i = 0}\\left(1 - P^D(x^i)\\right)\n",
    "\\prod_{i:\\theta^i \\neq 0}\\frac{P^D(x^i)g(z^{\\theta^i}|x^i)}{\\lambda_c(z^{\\theta^j})}\n",
    "\\\\ \\propto\n",
    "\\sum_{\\theta \\in \\Theta}\n",
    "\\prod_{i:\\theta^i = 0}\\left(1 - P^D(x^i)\\right)\n",
    "\\prod_{i:\\theta^i \\neq 0}\\frac{P^D(x^i)g(z^{\\theta^i}|x^i)}{\\lambda_c(z^{\\theta^j})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measurement model in case of linear Gaussian\n",
    "Assuming linear Gaussian measurement model $g(z|x) = \\mathcal{N}(z: Hx, R)$, constant probability of detection $P^D(x) = P^D$\n",
    "and uniform clutter intensity function $\\lambda_c(c) = \\frac{\\bar{\\lambda}_c}{V}$.\n",
    "\n",
    "$$\n",
    "p(Z|X) \\propto \\sum_{\\theta \\in \\Theta}(1 - P^D)^{(n - m^o)}\\prod_{i: \\theta^i \\neq 0}\\frac{P^D\\mathcal{N}(z^{\\theta^i}: Hx^i, R)}{\\bar{\\lambda}_c / V}\n",
    "$$\n",
    "\n",
    "The term $n - m^o$ can be understood as the amount of missdetected objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 2\n",
    "Z = np.array([[-1.6],[1]])\n",
    "m_o = len(Z)\n",
    "H = np.array([1.0])\n",
    "R = np.array([0.2])\n",
    "PD = 0.6\n",
    "rng = np.array([-5, 5])\n",
    "V  = np.diff(rng)\n",
    "lamc = lambda c: 0.3 if np.abs(c) <= 5 else 0\n",
    "lamc_bar = lamc(0) * V\n",
    "\n",
    "### REALLY INNEFICIENT WAY OF DOING IT, but does the job for ploting\n",
    "def gaussian_lm(theta, X, Z, H, R, PD, lamc):\n",
    "    n = len(Z)\n",
    "    m = len(Z)\n",
    "    m_o = np.sum(np.array(theta) != 0)\n",
    "    \n",
    "    f1 = np.exp(-lamc_bar) / math.factorial(m) # exponental part\n",
    "    f2 = np.prod([lamc(Z[j]) for j in range(m)]) # clutter part\n",
    "    p_miss = (1 - PD) ** (n - m_o) # misdetections part\n",
    "    f3 = np.prod([PD / lamc(Z[t-1]) for t in theta if t != 0]) # detections factor\n",
    "    \n",
    "    p_detect = np.prod([multivariate_normal(X[i], R).pdf(Z[t-1]) for i, t in enumerate(theta) if t!= 0]) # measurement likelihoods\n",
    "    return f1 * f2 * p_miss * f3 * p_detect\n",
    "\n",
    "def get_hyp_contour(theta, N=50):\n",
    "    N = 50\n",
    "    x = np.linspace(-5, 5, N)\n",
    "    x1, x2 = np.meshgrid(x, x)\n",
    "    pos = np.zeros((N,N,3))\n",
    "    pos[:,:,0] = x1\n",
    "    pos[:,:,1] = x2\n",
    "    \n",
    "    for t in theta:\n",
    "        for i in range(N):\n",
    "            for j in range(N):\n",
    "                X = np.array([[x[j]],[x[i]]])\n",
    "                pos[i, j, 2] += gaussian_lm(t, X, Z, H, R, PD, lamc)\n",
    "    return pos\n",
    "\n",
    "theta = [[0, 0],[1, 0],[0, 1],[2,0],[0,2],[2,1],[1,2]]\n",
    "pos = get_hyp_contour(theta, N=50)\n",
    "\n",
    "##\n",
    "fig = plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "plt.contourf(pos[:,:,0], pos[:,:,1], pos[:,:,2])\n",
    "plt.title(f\"Hypotheses: {theta}\")\n",
    "ax.set_xlabel(r\"$x_1$\")\n",
    "ax.set_ylabel(r\"$x_2$\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The symmetry in the measurement likelihood function comes from the unknown data association. We know that there can be measurements but dont know to which objects they belong. Another explanation could be that the hypothesis indexin is arbitrary, we are not forced to label the objects up to down, left to right or any other rule. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial prior for N objects\n",
    "There are two common possibilities to model the initial prior $p(X_0) = p(x_0^1,x_0^2,...,x_0^n)$ for $n$ objects and both of the rely on the assumption of initial independence and the assumed density filters. The possibilities in the asumed density are similar to SOT, namely, single Gaussian or less commonly a Gaussian mixture. This gives us two following factorizations of the joint density describing the initial prior:\n",
    "\n",
    "$$\n",
    "p(X_0) = p(x_0^1,x_0^2,...,x_0^n) = \n",
    "\\begin{cases}\n",
    "\\prod_{i=1}^n \\mathcal{N}(x_0^i: \\mu_0^i, P_0^i) &\n",
    "\\text{for Gaussian}\n",
    "\\\\\n",
    "\\sum_{h=1}^{H_0}w_0^h\\prod_{i=1}^n \\mathcal{N}(x_0^i: \\mu_0^i, P_0^i)\n",
    "& \\text{for Gaussian mixture}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior\n",
    "As for SOT and in general for problems under all Bayesian flitering framework the posterior is calculated using the Bayes rule. Given a prior and likelihood we similarly to SOT seek for a posterior expression as a sum over data associations factorizations into individual posteriors given association and association probabilities (or in other words weights):\n",
    "\n",
    "$$\n",
    "p(X_k|Z_{1:k}) \\propto p(Z_k|X_k)p(X_k|Z_{1:k-1}) = \\sum_{\\theta_k}p(X_k|Z_{1:k,\\theta_k})Pr[\\theta_k|Z_{1:k-1}]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Posterior from unimodal independent prior\n",
    "Consider the uni-modal independent prior:\n",
    "\n",
    "$$\n",
    "p(X) = \\prod_{i=1}^n p^i(x^i)\n",
    "$$\n",
    "\n",
    "Knowing the measurement likelihood from previous up to the proportionality constant we arrive at the following expression for the posterior:\n",
    "\n",
    "$$\n",
    "p(X|Z) \\propto \\sum_{\\theta \\in \\Theta}\n",
    "\\prod_{i:\\theta^i = 0}\\left(1 - P^D(x^i)\\right)\n",
    "\\prod_{i:\\theta^i \\neq 0}\\frac{P^D(x^i)g(z^{\\theta^i}|x^i)}{\\lambda_c(z^{\\theta^j})}\\prod_{i=1}^n p^i(x^i) \\\\ \n",
    "=\n",
    "\\sum_{\\theta \\in \\Theta}\n",
    "\\underbrace{\\prod_{i:\\theta^i = 0}\\left(1 - P^D(x^i)\\right)p^i(x^i)}_{\\text{misdetected}}\n",
    "\\underbrace{\\prod_{i:\\theta^i \\neq 0}\\frac{P^D(x^i)g(z^{\\theta^i}|x^i)}{\\lambda_c(z^{\\theta^j})}p^i(x^i)}_{\\text{detected}}\n",
    "$$\n",
    "\n",
    "This gives us the summation over the products corresponding to un-normalized posteriors for single mistedected and detected object densities. We can normalize misdetected and detected parts similarly SOT applying the same formulas for weights and pdfs. We get the following.\n",
    "\n",
    "$$\n",
    "p(X|Z) = \\sum_{\\theta \\in \\Theta}\\prod_{i=1}^n\\tilde{w}^{\\theta^i}p^{i,\\theta^i}(x^i)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\n",
    "\\theta^i = 0 \\Rightarrow\n",
    "\\begin{cases}\n",
    "\\tilde{w}^{\\theta^i} = \\int p^i(x^i)\\left(1 - P^D(x^i)\\right)dx^i\n",
    "\\\\\n",
    "p^{i,\\theta^i}(x^i) = \\frac{p(x^i)\\left(1 - P^D(x^i)\\right)}{\\tilde{w}^{\\theta^i}}\n",
    "\\end{cases}\n",
    "\\\\\n",
    "\\theta^i \\neq 0 \\Rightarrow\n",
    "\\begin{cases}\n",
    "\\tilde{w}^{\\theta^i} = \\frac{1}{\\lambda_c(z^{\\theta^i})}\\int p(x^i)P^D(x^i)g(z^{\\theta^i}|x^i)dx^i\n",
    "\\\\\n",
    "p^{i,\\theta^i}(x^i) = \\frac{p(x^i)P^D(x^i)g(z^{\\theta^i}|x^i)dx^i}{\\tilde{w}^{\\theta^i}}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The expression only normalizes the individual components and not the overall posterior $P(X|Z)$. Using the normalization the expression for the posterior factozizes into two products. We define product over weights over single object associations as the weight for $n$ object association $\\tilde{w}^{\\theta}$. Note, those weights are still unnormalized denoted by tilde. And the product over independent single object posterior densities is $n$ object posterior density $p^{\\theta}(X)$ conditioned on the data association $\\theta$.\n",
    "\n",
    "$$\n",
    "p(X|Z) \\propto \\sum_{\\theta \\in \\Theta}\\prod_{i=1}^n\\tilde{w}^{\\theta^i}\\prod_{i=1}^np^{i,\\theta^i}(x^i) = \n",
    "\\sum_{\\theta \\in \\Theta}\\tilde{w}^{\\theta}\\prod_{i=1}^np^{i,\\theta^i}(x^i) = \\sum_{\\theta \\in \\Theta}\\tilde{w}^{\\theta}p^{\\theta}(X)\n",
    "$$\n",
    "\n",
    "The weghts are normalized in a standard way:\n",
    "\n",
    "$$\n",
    "w^{\\theta} = \\frac{\\tilde{w}^{\\theta}}{\\sum_{\\theta}\\tilde{w}^{\\theta}} = \n",
    "\\frac{\\prod_i\\tilde{w}^{\\theta^i}}{\\sum_{\\theta}\\prod_i\\tilde{w}^{\\theta^i}}\n",
    "$$\n",
    "\n",
    "Same results can be acheived by calculating the normalization factor intergral from Bayes rule, but its not going to be shown here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Posterior from unimodal independent prior (linear Gaussians)\n",
    "Lets consider typical model with Gaussian prior and measurement likelihood. Also the probability of detection and clutter intensity are constant.\n",
    "$$\n",
    "p(X) = \\prod_{i=1}^n\\mathcal{N}(x^i:\\mu^i, P^i) \\\\\n",
    "p(Z|X) \\propto \\sum_{\\theta \\in \\Theta}(1 - P^D)^{n - m_o}\\prod_{i:\\theta_i \\neq 0}\\frac{P^DV}{\\bar{\\lambda_c}}\\mathcal{N}(z^{\\theta^i}:Hx^i, R)\n",
    "$$\n",
    "\n",
    "The posterior then becomes:\n",
    "\n",
    "$$\n",
    "p(Z|X) \\propto \\sum_{\\theta \\in \\Theta}\\prod_{i=1}^n(1 - P^D)\\mathcal{N}(x^i:\\mu^i, P^i)\n",
    "\\prod_{i:\\theta_i \\neq 0}\\frac{P^DV}{\\bar{\\lambda_c}}\\mathcal{N}(z^{\\theta^i}:Hx^i, R)\\mathcal{N}(x^i:\\mu^i, P^i)\n",
    "$$\n",
    "\n",
    "Our expression takes a form of factorization into two product terms that according to the previously learnt we can represent as $n$ object weight (everything that is independent on $x$) and $n$ posterior (dependent on $x$) given the hypothesis $\\theta$. Applying the individual factors normalization trick gives:\n",
    "\n",
    "$$\n",
    "p(X|Z) \\propto \\tilde{w}^{\\theta}\\prod_i\\mathcal{N}(x^i:\\mu^{i,\\theta^i}, P^{i,\\theta^i}) \\\\\n",
    "\\tilde{w}^{\\theta} = (1 - P^D)^{n - m_o}\\left(\\frac{P^DV}{\\bar{\\lambda_c}}\\right)^{m_o}\n",
    "\\prod_{i:\\theta_i \\neq 0}\\mathcal{N}(z^{\\theta^i}:\\hat{z}^i, S^i)\n",
    "$$\n",
    "\n",
    "The term $\\mathcal{N}(x^i:\\mu^{i,\\theta^i}, P^{i,\\theta^i})$ corresponds for a linear Kalman filter update for an object $i$ with the measurement $z^{\\theta^i}$ and the procedure goes as follows:\n",
    "\n",
    "1. If the object $i$ is detected meaning $\\theta^i \\neq 0$, get innovation  $y^{i,\\theta^i} = z^{\\theta^i} - \\hat{z}^i$ from measurement estimate $\\hat{z}^i = H\\mu^i$ and sensor measurement $z^{\\theta^i}$ associated with the object $i$.\n",
    "2. Get innovation covariance $S^i = HP^iH^T + R$\n",
    "3. get Kalman gain $K^i = P^iH^T(S^i)^-1$\n",
    "4. update the state estimate depending on whether the object is detected or not:\n",
    "$$\n",
    "\\mu^{i,\\theta^i} =\n",
    "\\begin{cases}\n",
    "\\mu^i + K^iy^{i,\\theta^i} & \\text{ if } \\theta^i \\neq 0 \\\\\n",
    "\\mu^i & \\text{ if } \\theta^i = 0\n",
    "\\end{cases}\n",
    "\\\\\n",
    "P^{i,\\theta^i} = \n",
    "\\begin{cases}\n",
    "P^i - K^iHP^i & \\text{ if } \\theta^i \\neq 0 \\\\\n",
    "P^i & \\text{ if } \\theta^i = 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The innovation and its covariance are also used in the predicted likelihood $\\mathcal{N}(z^{\\theta^i}:\\hat{z}^i, S^i)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maybe visualization here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General posterior\n",
    "Recap from SOT, general posterior expression looks as follows:\n",
    "$$\n",
    "p(x_k|Z_{1:k}) = \\sum_{\\theta_{1:k}}w^{\\theta_{1:k}}p^{\\theta_{1:k}}(x_{k}) \\: \\text{where:}\n",
    "\\\\\n",
    "\\sum_{\\theta_{1:k}} = \\sum_{\\theta_{1=0}}^{m_1}\\sum_{\\theta_{2=0}}^{m_2}...\\sum_{\\theta_{k=0}}^{m_k}\n",
    "$$\n",
    "The expression for the NOT looks very similar, the only difference is that we dont have a single data association hypothesis vector $\\theta_k$ at timestep $k$ but a set of valid hypotheses $\\theta_k \\in \\Theta_k$.\n",
    "$$\n",
    "p(X_k|Z_{1:k}) = \\sum_{\\theta_{1:k}}w^{\\theta_{1:k}}p^{\\theta_{1:k}}(X_{k}) \\: \\text{where:}\n",
    "\\\\\n",
    "\\sum_{\\theta_{1:k} \\in \\Theta_{1:k}} = \\sum_{\\theta_{1} \\in \\Theta_{1}}\\sum_{\\theta_{2} \\in \\Theta_{2}}...\\sum_{\\theta_{k} \\in \\Theta_{k}}\n",
    "$$\n",
    "\n",
    "After the first filter recursion the prior is not a unimodal function but a mixture of prior hypotheses $h$.\n",
    "$$\n",
    "p(x) = \\sum_h w^hp^h(X) = \\sum_hPr[h]p(X|h)\n",
    "$$\n",
    "\n",
    "This means that the posterior becomes a double sum over the prior hypotheses and current data associations. Applying the normalization we can define a new expression.\n",
    "$$\n",
    "p(X|Z) \\propto \\left[\\sum_{\\theta \\in \\Theta}p(Z,\\theta|X)\\right]\\left[\\sum_h w^hp^h(X)\\right] = \\sum_h\\sum_{\\theta \\in \\Theta}w^hp(Z,\\theta|X)p^h = \\sum_h\\sum_{\\theta \\in \\Theta}w^h\\tilde{w}^{\\theta|h}\\frac{p(Z,\\theta|X)p^h(X)}{\\tilde{w}^{\\theta|h}} = \\sum_h\\sum_{\\theta \\in \\Theta}\\tilde{w}^{h,\\theta}p^{h,\\theta}(X)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\n",
    "\\tilde{w}^{\\theta|h} = \\int p(Z,\\theta|X)p^h(X)dX = p(Z,\\theta|h)\n",
    "$$\n",
    "\n",
    "The normalized version of the mixture posterior is then:\n",
    "\n",
    "$$\n",
    "p(X|Z) = \\sum_h\\sum_{\\theta \\in \\Theta}\\tilde{w}^{\\theta|h}p^{\\theta|h}(X) = \\sum_h\\sum_{\\theta \\in \\Theta}Pr[h,\\theta]p(X|h,\\theta) \\\\\n",
    "w^{h,\\theta} = \\frac{w^{\\theta|h}w^h}{\\sum_h\\sum_{\\theta \\in \\Theta}w^{\\theta|h}w^h}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling motion of N objects\n",
    "We need to describe how the $n$ states evolve from $X_{k-1}^i$ to the next time step $x_k^i$ with a transition density.\n",
    "$$\n",
    "p(X_k|X_k) = p_k(x_k^1,x_k^2,...,x_k^n|x_{k-1}^1,x_{k-1}^2,...,x_{k-1}^n)\n",
    "$$\n",
    "And the simplifying assumption is that the time evolution of the objects is independent:\n",
    "$$\n",
    "p(X_k|X_k) = \\prod_{i=1}^n\\pi_k(x_k^n|x_{k-1}^n)\n",
    "$$\n",
    "In addition, typically the same transition density $\\pi_k$ is used for all objects but it is not the case. The indepence of the transition densities also means that each object is predicted independently of other objects and the prediction step for the single posterior becomes:\n",
    "$$\n",
    "p_{k|k-1}(X_k) = \\prod_{i}^{n}p_{k|k-1}^i(x_k^i)\n",
    "$$\n",
    "For the mixture posterior can predict individual components independently and the weights remain the same.\n",
    "$$\n",
    "p_{k|k-1}(X_k) = \\sum_h w_{k|k-1}^h p_{k|k-1}^h(X_k^i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian mixture posterior and linear Gaussian transition density case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the independent assumptions\n",
    "In our modelling we ofen assumed the independence condition for particular qualtits like:\n",
    "\n",
    "1. Initial prior density\n",
    "2. Measurements\n",
    "3. Object motion\n",
    "\n",
    "Does it also mean that the object states are independent? This is not true, in the general case at least. The objects are independent for a single hypothesis, however the total posterior is the mixture represented by the summation over all hypotheses therefore:\n",
    "\n",
    "$$\n",
    "p(X) = \\sum_h w^h \\prod_{i=1}^n p^{i,h}(x^i) \\neq \\prod_{i=1}^n\\sum_{h^i}w^{h^i}p^{i,h^i}(x^i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data association optimization\n",
    "One of the biggeste problems in our recursive algorithm is rapidly growing number of data associations. Just after one recursion with $h$ hypotheses we get $N_A(m,n)$ new hypotheses for every hypothesis. We want to find a subset of data associations that is way smaller than the original set and has only the largest weights. We also want to avoid computing all valid hypotheses and compare their weights. A solution is to pose this problem a special optimization problem called **assignment problem**. \n",
    "\n",
    "#### Assignment problem\n",
    "Three workers $w_1, w_2, w_3$ and three tasks $t_1, t_2, t_3$ each worker gives a cost for each task as follows:\n",
    "\n",
    "| $w/t$ | $t_1$ | $t_2$ | $t_3$ |\n",
    "| --- | --- | --- | --- |\n",
    "| $w_1$ | $5$ | $8$ | $7$ |\n",
    "| $w_2$ | $8$ | $12$ | $7$ |\n",
    "| $w_3$ | $4$ | $8$ | $5$ |\n",
    "\n",
    "Each worker can only take 1 task and each task can only be solved by 1 worker. We want to assign workers to tasks such that each worker has something to do, each task is performed and the most importantly the cost is as low as possible. We can also assign tasks to workers and the same assignment will be optimal. Later we would want to do the same with $n$ objects and $m$ measurements and define a special cost for them.\n",
    "\n",
    "The matrix presented above is the cost matrix $L$ and the assignment matrix $A$ is the matrix of same size that reprsents assignments with 1 and the rest of the fields with 0. The cost of an assignment is then represented as:\n",
    "\n",
    "$$\n",
    "C = \\sum_i \\sum_j A^{i,j}L^{i,j} = tr(A^TL)\n",
    "$$\n",
    "\n",
    "Then the optimal assignment problem reads as follows. Given $L$ we seek the solution $A^*$ to the constrained minimization problem:\n",
    "\n",
    "- minmize $tr(A^TL)$\n",
    "- subject to $A^{i,j} \\in \\{0,1\\}, \\: \\forall i,j$ (either assign or not)\n",
    "- $\\sum_j A^{i,j} = 1, \\: \\forall i$ (Each worker $i$ assigned one task $j$)\n",
    "- $\\sum_i A^{i,j} = 1, \\: \\forall j$ (Each task $j$ assigned one worker $i$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Object tracking assigment matrix A\n",
    "For a given hypothesis the optimal data association has the maximum weight which can be described as the product of the weights for each object. The maximization of the product can be re-written as the maximization of the sum of logarithms. In addition the maximization can be posed as negative minimization whic yields that the optimal data association $\\theta*$ is a solution to the following problem:\n",
    "\n",
    "$$\n",
    "\\theta^* = arg\\min_{\\theta \\in \\Theta}\\sum_{i=1}^n-log(\\tilde{w}^{\\theta^i|h})\n",
    "$$\n",
    "\n",
    "The assignment matrix $A$ for $n$ object tracking should have a structure that corresponds to **unique valid associations** $\\theta \\in \\Theta$. $A$ is a $n \\times (m+n)$ matrix that assigns $n$ objects to $m$ detections and $n$ misdetections. The realationship between $\\theta$ and $A$ is:\n",
    "\n",
    "- Detection: $\\theta^i = j \\Rightarrow A^{i,j} = 1$\n",
    "- Misdetection: $\\theta_i = 0 \\Rightarrow A^{i,m+i} = 1$\n",
    "\n",
    "For example, $n=2$, $m=1$, $N_A(1,2)=3$:\n",
    "$$\n",
    "\\theta=[0, 0] \\Rightarrow A=\n",
    "\\left[\n",
    "\\begin{array}{c|cc}\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1 \\\\\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\theta=[1, 0] \\Rightarrow A=\n",
    "\\left[\n",
    "\\begin{array}{c|cc}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 0 & 1 \\\\\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\theta=[0, 1] \\Rightarrow A=\n",
    "\\left[\n",
    "\\begin{array}{c|cc}\n",
    "0 & 1 & 0 \\\\\n",
    "1 & 0 & 0 \\\\\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Object tracking cost matrix L\n",
    "Applying the definition of weights from previous to our hypothesis optimization problme we get the following entries for the cost matrix:\n",
    "\n",
    "- log-likelihood of misdetecting $x^i$: $l^{i,0,h} = \\log\\left( \\int \\left(1 - P^D(x^i)\\right) p^{i,h}(x^i)dx^i\\right)$\n",
    "- log-likelihood of associating $x^i$ to $z^i$: $l^{i,j,h} = \\log\\left( \\int \\frac{P^D(x^i)}{\\lambda_c(z^j)}(x^i)g(z^j|x^i)p^{i,h}(x^i)dx^i\\right)$\n",
    "\n",
    "Applying those equations to the simple scenario of Gaussian prior, linear Gaussian object measurement likelihood, constant probability of detection and uniform clutter intensity function we get:\n",
    "\n",
    "$$\n",
    "l^{i,0,h} = \\log\\left(1 - P^D\\right) \\\\\n",
    "l^{i,j,h} = \\log\\left(\\frac{P^DV}{\\bar{\\lambda}}\\right)\n",
    "- \\frac{1}{2}\\log\\left(\\det(2\\pi S^{i,h})\\right)\n",
    "- \\frac{1}{2}\\left(z^j - \\hat{z}^{i,h}\\right)(S^{i,h})^{-1}\\left(z^j - \\hat{z}^{i,h}\\right)\n",
    "$$\n",
    "\n",
    "L will take a shape $n \\times (m + n)$ with the same structure as the assignment matrix. The difference is that the off-diagonal terms in the misdetected part of the matrix are $\\infty$ instead of $0$ to ensure that each invalid detection is heavily taxed by infinite cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example\n",
    "mention params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4  5  8]\n",
      " [ 8  8 12]\n",
      " [ 5  7  7]]\n"
     ]
    }
   ],
   "source": [
    "Calculate L, calculate 3 costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization solvers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gating to reduce the optimization complexity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
